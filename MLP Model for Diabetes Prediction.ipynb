{"nbformat":4,"nbformat_minor":0,"metadata":{"_change_revision":0,"_is_fork":false,"accelerator":"GPU","colab":{"name":"MLP Model for Diabetes Prediction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dujqAogI8bQZ"},"source":["### Loading of different packagaes and APIs"]},{"cell_type":"code","metadata":{"id":"Xz80SgiJtd5o","executionInfo":{"status":"ok","timestamp":1613906875120,"user_tz":-330,"elapsed":869,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":["import numpy as np \n","np.random.seed(6)\n","import random\n","random.seed(6)\n","# from keras import backend as K\n","import tensorflow\n","tensorflow.random.set_seed(6)\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","# sns.set(style=\"whitegrid\")\n","import warnings \n","from sklearn.svm import SVC, NuSVC\n","warnings.filterwarnings('ignore')\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.neighbors  import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy import stats\n","from scipy.stats import uniform, randint\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n","from sklearn.metrics import roc_curve, auc, accuracy_score\n","# from tflearn.data_utils import to_categorical\n","from sklearn import preprocessing\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import classification_report\n","from scipy import interp\n","from sklearn.metrics import confusion_matrix\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import FastICA\n","from keras.utils import to_categorical\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from sklearn.model_selection import GridSearchCV, KFold\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.optimizers import Adam\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import KFold\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Activation, Dense, Dropout, BatchNormalization, Input\n","from keras.models import Model\n","from keras.optimizers import Adam\n"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d272noMSoezZ"},"source":["### Utility Functions"]},{"cell_type":"code","metadata":{"id":"B5ce3ePzKLDP","executionInfo":{"status":"ok","timestamp":1613906883776,"user_tz":-330,"elapsed":6674,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":["Renamed_feature= []               #list of names that will rename to feature column\n","all_clf_res=[]                    #every classifier auc values are stored in it\n","random_initializer=20            #random initializer\n","n_dots=50\n","##########################################################\n","\n","for i in range(8):\n","  #for renaming dataset of columns features F1 -- F8 \n","  Renamed_feature.append('F'+str(i+1)) \n","############################################################\n","\n","# Pairs plots are just showing all variables paired with all the other variables\n","def pair_plot(data):\n","  '''\n","  This function will create a grid of Axes such that each variable\n","  in data will by shared in the y-axis across a single row and in the x-axis\n","  across a single column.The diagonal Axes are treated differently, drawing \n","  a plot to show the univariate distribution of the data for the variable in\n","  that column.\n","  Parameters :\n","  Input - data is the pandas type variable for \n","  plotting pair plot of features in this\n","  dataframe \n","  \n","  Output :\n","  This function Plot pairwise relationships in a dataset.\n","  \n","  '''\n","  plt.figure()\n","  \n","  pair_plot =sns.pairplot(data=data,            \n","                          height=3,\n","                          hue='Outcome',  \n","                          diag_kind='kde')\n","  # fig.suptitle(\"Pairplot of all features\")\n","  pair_plot.fig.suptitle(\"Pairplot of all features\")\n","  plt.show()\n","\n","\n","\n","\n","###################################################################\n","# this function for Gaussian distribution plot \n","# and box plot simultaneously in a figure\n","def Box_Gaussian(data):\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Output - The Gaussian distribution plot for\n","  eight feature of input data \n","\n","  '''\n","  \n","  fig=plt.figure(figsize=(15,6))                                #define figure size\n","  fig.suptitle(\"Box Gaussian plot of all features\")\n","  \n","  n_scaler = preprocessing.StandardScaler()                 #standardization function\n","  temp_Data = n_scaler.fit_transform(data)                  #pass into function for standrd.\n","  for i in range(8):                                        #loop for all 8 feature \n","    \n","    plt.subplot(2, 4, i+1)                                  #subplot for 2 rows in 4 columns\n","    Data = temp_Data[:,i]                                   #data for every feature\n","    sns.kdeplot(Data, shade=True,color='red', alpha=0.3)    #kernel density function under red shaded arae\n","    ax = sns.boxplot(Data, saturation=0.9, color=\"green\")   #boxplot  with green shaded area\n","                                                            # https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n","                                                            # https://seaborn.pydata.org/generated/seaborn.boxplot.html\n","    plt.gca().invert_yaxis()                                #Reverse Y-Axis in PyPlot\n","    # plt.title('F'+str(i+1))\n","    frame1 = plt.gca()\n","    frame1.axes.xaxis.set_ticklabels([])                    #removing xlabel data \n","    plt.ylim((-0.5,0.65))                                   #y axis  limit\n","    plt.tight_layout()                                      #This module provides routines to adjust subplot params so that subplots are nicely fit in the figure.\n","                                                            # https://matplotlib.org/api/tight_layout_api.html\n","    # plt.grid('on')\n","    \n","    for patch in ax.artists:\n","      r, g, b, a = patch.get_facecolor()                     #Get the facecolor of the Axes.\n","      patch.set_facecolor((r, g, b, 0.3))                    #set colour intensity\n","#############################################################\n","\n","\n","def plot_confusionMatrix(data):\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Output -visualization of correalation matrix of\n","  input data \n","\n","  '''\n","  sns.set(font_scale=1.15)                                    # Set aesthetic parameters in one step.\n","  ax = plt.figure(figsize=(10, 8))                            #set figure size   https://seaborn.pydata.org/generated/seaborn.set.html\n","  plt.title(\"Confusion Matrix of all features\")\n","  sns.heatmap(data.corr(),                                    # input correlation matrix  of dataset\n","              vmax=1.0,                                       #Values to anchor the colormap, otherwise they are inferred from\n","                                                              #the data and other keyword arguments.\n","              vmin=0.0,\n","              linewidths=0.01,\n","              square=False,                                   #If True, set the Axes aspect to “equal” so each cell will be square-shaped.\n","              annot=True,                                     #If True, write the data value in each cell. \n","              linecolor=\"black\")                              #Color of the lines that will divide each cell.            \n","                                                              #https://seaborn.pydata.org/generated/seaborn.heatmap.html\n","  b, t = plt.ylim()                                           # discover the values for bottom and top\n","  b += 0.5                                                    # Add 0.5 to the bottom\n","  t -= 0.5                                                    # Subtract 0.5 from the top\n","  plt.ylim(b, t)                                              # update the ylim(bottom, top) values\n","  plt.show() \n","\n","\n","\n","############################################################\n","# this function plot univariate distribution of \n","# every feature\n","\n","def dist_Plot(data):\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Output - The distribution plot for\n","  eight feature of input data \n","\n","  '''\n","  fig, ax = plt.subplots(2,4, figsize=(12,5))                 #set numbers of rows and columns of subplot\n","  sns.set()\n","  sns.distplot(data.F1, bins = 10, ax=ax[0,0])                #Flexibly plot a univariate distribution of observations.\n","  sns.distplot(data.F2, bins = 10, ax=ax[0,1]) \n","  sns.distplot(data.F3, bins = 10, ax=ax[0,2]) \n","  sns.distplot(data.F4, bins = 10, ax=ax[0,3])\n","  sns.distplot(data.F5, bins = 10, ax=ax[1,0]) \n","  sns.distplot(data.F6, bins = 10, ax=ax[1,1]) \n","  sns.distplot(data.F7, bins = 10, ax=ax[1,2]) \n","  sns.distplot(data.F8, bins = 10, ax=ax[1,3]) \n","  fig.suptitle(\"Gaussian Distribution of all features\")\n","  fig.tight_layout()                                          #This module provides routines to adjust subplot params\n","                                                              #  so that subplots are nicely fit in the figure.\n","                                                        \n","\n","############################################################\n","# this function plot violin plot  of \n","# every feature\n","\n","\n","def plot_violinplot (data):\n","\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Output - The violinplot plot for\n","  eight feature of input data \n","\n","  '''\n","\n","  # A violin plot is a method of plotting numeric data.\n","  # It is similar to box plot with a rotated kernel \n","  # density plot on each side. Violin plots are similar\n","  # to box plots, except that they also show the probability\n","  # density of the data at different values (in the simplest\n","  # case this could be a histogram).\n","  fig, ax = plt.subplots(2,4, figsize=(12,6))          \n","  # #set numbers of rows and columns of subplot and figure size \n","  sns.set()\n","  sns.violinplot(x = data.Outcome, y=data.F1,  ax=ax[0,0])    #violine plot for F1 feature\n","  sns.violinplot(x = data.Outcome, y=data.F2,  ax=ax[0,1])    #violine plot for F2 feature \n","  sns.violinplot(x = data.Outcome, y=data.F3,  ax=ax[0,2])    #violine plot for F3 feature \n","  sns.violinplot(x = data.Outcome, y=data.F4,  ax=ax[0,3])    #violine plot for F4 feature\n","  sns.violinplot(x = data.Outcome, y=data.F5,  ax=ax[1,0])    #violine plot for F5 feature \n","  sns.violinplot(x = data.Outcome, y=data.F6,  ax=ax[1,1])    #violine plot for F6 feature \n","  sns.violinplot(x = data.Outcome, y=data.F7,  ax=ax[1,2])    #violine plot for F7 feature \n","  sns.violinplot(x = data.Outcome, y=data.F8,  ax=ax[1,3])    #violine plot for F8 feature \n","  fig.suptitle(\"Violin plot of all features\")\n","  fig.tight_layout()\n","\n","                                                              # https://seaborn.pydata.org/generated/seaborn.violinplot.html\n","\n","############################################################\n","\n","\n","#this function  is for manual outleir rejection\n","def Manual (data):\n","\n","    '''\n","    Parameters :\n","    Input - data is the pandas type variable\n","\n","    Return - dataframe with outleir rejection\n","    of input data \n","\n","    '''\n","    # input dataset is data \n","    max_Pregnancies = data.F1.max()                         #maximum feature of F1\n","    data = data[data.F1!=max_Pregnancies]                   #find  where extreme value is absent and remove extreme\n","    max_Glucose = data.F2.max()                             #maximum feature of F2  \n","    data = data[data.F2!=max_Glucose]                       #find  where extreme value is absent and remove\n","    for i in range(4):                                      #in this loop we succesively remove 4 minimum element \n","      min_Glucose = data.F2.min()                           #find minimum\n","      data = data[data.F2!=min_Glucose]                     #reject minimum\n","    max_BloodPressure = data.F3.max()                       #maximum feature of F3\n","    data = data[data.F3!=max_BloodPressure]                 #find  where extreme value is absent and remove\n","    for i in range(2):                                      #in this loop we succesively remove 2 extreme element  \n","      max_skinthickness = data.F4.max() \n","      data = data[data.F4!=max_skinthickness]\n","    for i in range(25):                                     #in this loop we succesively remove 25 extreme element  \n","      max_Insulin = data.F5.max() \n","      data = data[data.F5!=max_Insulin]\n","    max_bmi = data.F6.max()\n","    data = data[data.F6!=max_bmi]\n","    for i in range(4):                                      #in this loop we succesively remove 4 minimum element  \n","      min_bmi = data.F6.min() \n","      data = data[data.F6!=min_bmi]\n","    for i in range(20):                                     #in this loop we succesively remove 20 extreme element \n","      max_DiabetesPedigreeF = data.F7.max()\n","      data = data[data.F7!=max_DiabetesPedigreeF]\n","    for i in range(20):                                     #in this loop we succesively remove 20 extreme element  \n","      max_age = data.F8.max() \n","      data = data[data.F8!=max_age]\n","      df =data\n","    return data\n","\n","############################################################\n","\n","# this function if for outlair rejection with\n","# respect to mean value\n","def IQR_Mean (data):\n","\n","\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Return - dataframe with outleir rejection\n","  filled with mean\n","  of input data \n","\n","  '''\n","  for i in range(8): \n","    x = data[Renamed_feature[i]]\n","    Q1 = x.quantile(0.25)                                   # Q1 is the \"middle\" value in the first half of the rank-ordered data set.\n","    Q3 = x.quantile(0.75)                                   # Q3 is the \"middle\" value in the second half of the rank-ordered data set.\n","    IQR = Q3-Q1                                             # The interquartile range is equal to Q3 minus Q1.\n","    mean = x.mean()                                         #mean of feature \n","    for j in range(569):                                    # loop for first 569 elements of feature\n","      temp = x[j]                                           # every feature value\n","      LW = (Q1 - 1.5 * IQR)                                 #lower considerable range of gaussian distribution\n","      UW = (Q3 + 1.5 * IQR)                                 #upper considerable range of gaussian distribution\n","      if temp < LW:                                         #replace upper value with mean\n","        x[j] = mean\n","      if temp > UW:                                         #replace lower value with mean\n","        x[j] = mean\n","    data[Renamed_feature[i]] = x\n","  return data\n","\n","############################################################\n","# this function if for outlair rejection with\n","# respect to median value same as previous function\n","def IQR_Median (data): \n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Return - dataframe with outleir rejection\n","  filled with median\n","  of input data \n","\n","  '''\n","  for i in range(8):\n","    x = data[Renamed_feature[i]]\n","    Q1 = x.quantile(0.25)\n","    Q3 = x.quantile(0.75)\n","    IQR = Q3-Q1\n","    median = x.quantile(0.5)                                # find the median\n","    for j in range(569):                                    #replace the first 569 values with respect to median\n","      temp = x[j]\n","      LW = (Q1 - 1.5 * IQR)\n","      UW = (Q3 + 1.5 * IQR)\n","      if temp < LW:                                         #replace upper value with median\n","        x[j] = median\n","      if temp > UW:\n","        x[j] = median                                       #replace upper value with median\n","    data[Renamed_feature[i]] = x\n","  return data\n","\n","############################################################\n","\n","\n","\n","# this function if for outlair rejection with\n","# 1.5 times of IQR that means that are\n","# significant in gaussian distribution\n","def IQR (data):\n","\n","  '''\n","  Parameters :\n","  Input - data is the pandas type variable\n","\n","  Return - dataframe with outleir rejection\n","  of input data \n","\n","  '''\n","  #input dataset as data\n","  for i in range(8):                                        # for every feature\n","    Q1 = data[Renamed_feature[i]].quantile(0.25)\n","    Q3 = data[Renamed_feature[i]].quantile(0.75)\n","    IQR = Q3-Q1                                             #find IQR\n","    LW = (Q1 - 1.5 * IQR)                                   #find lower boundary\n","          # print(LW)\n","    UW = (Q3 + 1.5 * IQR)                                   #find upper boundary\n","          # print(UW)\n","    data = data[data[Renamed_feature[i]]<UW]                #drop greater than upper limit\n","    data = data[data[Renamed_feature[i]]>LW]                #drop smaller than lower limit\n","\n","  return data\n","\n","\n","############################################################\n","#outlier rejection with different condition\n","\n","def outlier_Rejection (data, iqr_Mean, iqr_Medain, iqr, manual):\n","  '''\n","  Parameters :\n","  Input - \n","  data is the pandas type variable\n","  iqr_Mean - for outleir rejection with Mean\n","  iqr_Medain- for outleir rejection with Medain\n","  iqr- for drop the outleir \n","  manual -for manual rejection \n","  Return - dataframe with outleir rejection\n","  filled with Input parameter\n","\n","  '''\n","  \n","  # outlier_Rejection with conditional input\n","  if iqr_Mean == True:                                     #reject outleir with Mean\n","    data = IQR_Mean (data)\n","  if iqr_Medain == True:                                   #reject outleir with Median\n","    data = IQR_Medain (data)\n","  if iqr == True:                                          #reject outleir in IQR range\n","    data = IQR (data)\n","  if manual == True:                                       #reject outleir with manual\n","    data = Manual (data)\n","\n","  return data\n","\n","############################################################\n","\n","#data plot on different input condition \n","def data_plot (data,\n","               Pair_plot,\n","               Dist_Plot,\n","               Plot_violinplot,\n","               Plot_confusionMatrix,\n","               box_Gaussian):\n","  \n","  '''\n","  Parameters :\n","  Input - \n","  data - It is the pandas type variable\n","  Pair_plot - for pair plot visualization of input  data  \n","  Dist_Plot- for gaussian distribution plot visualization of input  data  \n","  Plot_violinplot- for violin plot visualization of input  data  \n","  Plot_confusionMatrix -for confusion matrix visualization of input  data   \n","  \n","  Output - dataframe with outleir rejection\n","  filled with Input parameter\n","\n","  '''\n","  if Pair_plot ==True:\n","    pair_plot(data)\n","\n","  if Dist_Plot ==True:\n","    dist_Plot(data)\n","\n","  if Plot_violinplot ==True:\n","    plot_violinplot (data)\n","\n","  if Plot_confusionMatrix ==True:\n","    plot_confusionMatrix(data)\n","\n","  if box_Gaussian ==True:\n","    Box_Gaussian(data)\n","\n","\n","############################################################\n","def replace_zero(data, field, target):\n","    \n","    mean_by_target = data.loc[data[field] != 0, [field, target]].groupby(target).mean()\n","    data.loc[(data[field] == 0)&(data[target] == 0), field] = mean_by_target.iloc[0][0]\n","    data.loc[(data[field] == 0)&(data[target] == 1), field] = mean_by_target.iloc[1][0]\n","\n","\n","############################################################\n","def metrics (y_true, y_pred, probas_):\n","\n","\n","  '''\n","  Parameters :\n","  Input - \n","  y_true - true  value of input data    \n","  y_pred- predicted  value of input data  \n","  probas_- probability/confidence of predicted output\n","\n","  return -True Negative(tn),False Positive(fp),False Negative(fn)\n","  True positive(tp),AUC(roc_auc),False Positive Rate(fpr),\n","  True positive rate(tpr)\n","\n","  '''\n","\n","\n","  points=n_dots*'-'\n","  print(points)\n","#    print(\"Best parameters set found on development set:\")\n","#    print(clf.best_params_)\n","  fpr, tpr, thresholds = roc_curve(y_true, probas_[:, 1])\n","  tprs.append(interp(mean_fpr, fpr, tpr))\n","  tprs[-1][0] = 0.0\n","  roc_auc = auc(fpr, tpr)\n","  #  aucs.append(roc_auc)\n","  print(\"Detailed classification report for current fold:\")\n","  print()\n","  print(classification_report(y_true, y_pred))\n","  print()\n","  print(\"Area Under ROC (AUC): {}\".format(roc_auc))\n","  print()\n","  print('Confusion Matrix for current fold: ')\n","  print(confusion_matrix(y_true, y_pred))\n","  print()\n","  print(\"Accuracy for Current Fold: {}\".format(accuracy_score(y_true, y_pred)))\n","  print()\n","  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","  return  tn, fp, fn, tp, roc_auc, fpr, tpr\n","\n","############################################################\n","\n","\n","def average_ROC(mean_fpr,tprs,aucs,TP,TN,FP,FN):\n","\n","  '''\n","  Parameters :\n","  mean_fpr - Mean False positive rate\n","  tprs -values of true positive rate\n","  aucs  - values of auc\n","  TP    - True positive \n","  TN    - True Negative\n","  FP    - False Positiv\n","  FN    - False Negative\n","\n","  Output - \n","  Visalization of TPR vs FPR plot\n","  '''\n","  sen = (np.sum(TP))/(np.sum(TP)+np.sum(FN))\n","  spe = (np.sum(TN))/(np.sum(TN)+np.sum(FP))\n","\n","  mean_tpr = np.mean(tprs, axis=0)\n","  mean_tpr[-1] = 1.0\n","  # mean_auc = auc(mean_fpr, mean_tpr)\n","  mean_auc = np.mean(aucs)\n","  std_auc = np.std(aucs)\n","  # plt.figure(figsize=(8, 5))\n","  # plt.grid(True)\n","  ax = plt.axes()\n","  ax.grid(color='lightgray', linestyle='-', linewidth=.5)\n","  # Setting the background color\n","  ax.set_facecolor(\"white\")\n","  \n","  ax.spines['bottom'].set_color('#000000')\n","  ax.spines['top'].set_color('#000000') \n","  ax.spines['right'].set_color('#000000')\n","  ax.spines['left'].set_color('#000000')\n","\n","  plt.plot(mean_fpr, mean_tpr, color='blue',\n","          label=r'Avg. ROC (AUC (avg $\\pm$ std) = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),\n","          lw=2, alpha=.8)\n","  \n","  plt.scatter((1-spe), sen, s=80, c='r', marker='x',)\n","  plt.scatter(0, sen, s=80, c='r', marker='x',)\n","  plt.scatter((1-spe),0, s=80, c='r', marker='x',)\n","  plt.axhline(y=sen, color='r', linestyle='--')\n","  plt.axvline(x=(1-spe), color='r', linestyle='--')\n","  plt.text((1-spe), 0.02, \"FPR={:0.3f}\".format((1-spe)))\n","  plt.text(0.009, sen+0.05, \"TPR={:0.3f}\".format(sen))\n","\n","  std_tpr = np.std(tprs, axis=0)\n","  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","  plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='darkgray', alpha=0.5,\n","                  label=r'$\\pm$ 1 Standard deviation')\n","\n","  plt.xticks(np.arange(0.0, 1.01, step=0.1))\n","  plt.yticks(np.arange(0.0, 1.01, step=0.1))\n","  left=0.0\n","  right=1.0\n","  plt.xlim(left, right)\n","  plt.ylim(left, right)\n","  plt.xlabel('False Positive Rate (FPR)')\n","  plt.ylabel('True Positive Rate (TPR)')\n","  plt.legend(loc=\"lower right\")\n","  # plt.grid(True)\n","  plt.show()\n","   \n","############################################################\n","def plot_Current_ROC(fpr,tpr,iterator,roc_auc):\n","  \n","  '''\n","  Parameters :\n","  Input - \n","  fpr - False positive rate\n","  tpr - True positive rate\n","  roc_auc -auc values of roc curve\n","\n","  Output - \n","  Visalization of current roc curve\n","\n","  '''\n","  plt.plot(fpr,\n","          \n","          tpr,\n","          # Color[iterator],\n","          alpha=0.35,\n","          # label='macro-average ROC (AUC = {0:0.3f})'.format(roc_auc)\n","          # +FOLD[iterator],\n","          linewidth=1)\n","   \n","############################################################\n","def creat_Model (classifier, X_Train, Y_Train, tuned_parameters, verbose):\n","\n","  '''\n","  Parameters :\n","  Input - \n","  X_Train -train data \n","  Y_Train - label/output of train data\n","  tuned_parameters =parameters of models\n","  verbose = condition about model\n","\n","  Output - \n","  Returned a tuned classifier using grid search\n","  '''\n","  clf = GridSearchCV(classifier,\n","                    tuned_parameters,\n","                    verbose=verbose,\n","                    cv=5,\n","                    scoring='roc_auc',\n","                    n_jobs=-1)\n","  clf.fit(X_Train, Y_Train)\n","  return clf\n","############################################################\n","\n","def average_performance(aucs,Accuracy,TP,TN,FP,FN): \n","\n","  '''\n","  Parameters :\n","  Input - \n","  aucs= values of aucs\n","  Accuracy - value of accuracy\n","  TP  - True Positive\n","  TN  - True Negative\n","  FP  - False Positive\n","  FN  - False Negative\n","\n","\n","  Output - \n","  It prints the average aucs,accuarcy,confusion matrix\n","  '''\n","\n","  print()\n","  n_dotsav=(n_dots-len('Average'))//2\n","    \n","  print('-'*n_dotsav+'Average'+'-'*n_dotsav)\n","  print(\"AUC (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(aucs),np.std(aucs)))\n","  print(\"Accuracy (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(Accuracy),np.std(Accuracy)))\n","  cm = [[int(np.mean(TP)), int(np.mean(FP))],[int(np.mean(FN)), int(np.mean(TN))]]\n","  print ('Avg. CM is '+str(cm))\n","  cm = [[int(np.sum(TP)), int(np.sum(FP))],[int(np.sum(FN)), int(np.sum(TN))]]\n","  print ('Total for all folds CM is '+str(cm))\n","  re_auc=str(round(np.mean(aucs), 3))+'+/-'+str(round(np.std(aucs),3))\n","  all_clf_res.append(re_auc)\n","    \n","############################################################\n","#this  function is for algorithm based feature selection \n","def feature_Selector(data, algo, n_feature):\n","    '''\n","    Parameters :\n","    Input - \n","    data - It is the pandas type variable\n","    algo - type of algorith PCA,ICA,Correlation\n","\n","\n","    Output - \n","    It prints the average aucs,accuarcy,confusion matrix\n","    '''\n","    if algo=='PCA':                                                   #for pca algorithm\n","        X_Data= data.iloc[:,:8].values\n","        pca = PCA(n_components=n_feature)                             #number of feature\n","        X_Data = pca.fit_transform(X_Data)\n","        return X_Data , data.iloc[:,8:].values\n","\n","    if algo == 'ICA':\n","        X_Data= data.iloc[:,:8].values\n","        ICA = FastICA(n_components=n_feature, random_state=12) \n","        X_Data = ICA.fit_transform(X_Data)\n","        return X_Data , data.iloc[:,8:].values\n","    \n","    if algo =='corr':                                                   #for ica algorithm\n","        if n_feature ==4:\n","            data = data[['F2','F5','F4','F6','Outcome']]                #for 4 feature\n","            return data.iloc[:,:4].values, data.iloc[:,4:].values\n","        if n_feature ==6:\n","            data = data[['F1','F2','F4','F5','F6','F8','Outcome']]       #for 6 feature\n","            return data.iloc[:,:6].values, data.iloc[:,6:].values\n","        \n","    if algo == 'None':\n","        return data.iloc[:,:8].values, data.iloc[:,8:].values            #if feature selection is off all features are counted\n","    \n","    \n","def run (hLayer,\n","         batch_size = [32],           \n","         epochs = [100],              \n","         learn_rate = [0.001],         \n","         dropout_rate = [0.3],       \n","         activation = ['relu'],   \n","         init =['normal']):\n","    \"\"\"\n","    Input : hidden layer number \n","    and others are alwalys constant\n","    \n","    Output : optimized model with best number of neuron \n","    in every layer \n","    \"\"\"\n","    \n","    \n","    if hLayer==1:  \n","        \n","        ###########################################\n","        #  number of neurons  for every layer\n","        #  optimization\n","        neuron1 = [16, 32, 64] \n","        neuron2 = [16, 32, 64]\n","        ############################################\n","        \n","        \n","        #############################################\n","        # parameter dictionary for grid search#\n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2)\n","        ###############################################\n","        \n","        ###############################################\n","        # model building function for given parameters#\n","        def nn_opt_1(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2):\n","            \"\"\"\n","            Input : activation ,dropout_rate,init,learn_rate,neuron1,neuron2\n","            \n","            Output : Using input hyper_parameters build model and return it\n","            \"\"\"\n","            \n","            #####################################################################\n","            ##               model initialization and building block           ##\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            \n","            #####################################################################\n","            \n","            return model\n","        \n","        ########################### grid search for optimisaion #############################  \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_1, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        ########################################################################################\n","        \n","        ########################################################################################\n","        ##                  build model with optimized parameters                             ##\n","        model =nn_opt_1(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'])\n","        ########################################################################################\n","        \n","        #  return best parameters and model\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    \n","    if hLayer==2:       \n","        neuron1 = [16, 32, 64]             \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]   \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3)\n","        \n","        def nn_opt_2(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_2, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_2(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==3:       \n","        neuron1 = [16, 32, 64]              \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]  \n","        neuron4 = [16, 32, 64]  \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4)\n","        \n","        def nn_opt_3(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_3, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_3(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==4:       \n","        neuron1 = [16, 32, 64]              \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]   \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5)\n","        \n","        def nn_opt_4(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_4, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_4(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==5:       \n","        neuron1 = [16, 32, 64]             \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64] \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6)\n","        \n","        def nn_opt_5(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_5, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_5(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","\n","    if hLayer==6:       \n","        neuron1 = [16, 32, 64]              \n","        neuron2 = [16, 32, 64]               \n","        neuron3 = [16, 32, 64]  \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        neuron7 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6,\n","                          neuron7=neuron7)\n","        \n","        def nn_opt_6(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6,\n","                   neuron7):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_6, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_6(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'],\n","                grid_results.best_params_['neuron7'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==7:       \n","        neuron1 = [16, 32, 64]               \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]   \n","        neuron4 = [16, 32, 64] \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        neuron7 = [16, 32, 64] \n","        neuron8 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6,\n","                          neuron7=neuron7,\n","                          neuron8=neuron8)\n","        \n","        def nn_opt_7(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6,\n","                   neuron7,\n","                   neuron8):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_7, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_7(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'],\n","                grid_results.best_params_['neuron7'],\n","                grid_results.best_params_['neuron8'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==8:       \n","        neuron1 = [16, 32, 64]              \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]   \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        neuron7 = [16, 32, 64] \n","        neuron8 = [16, 32, 64] \n","        neuron9 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6,\n","                          neuron7=neuron7,\n","                          neuron8=neuron8,\n","                          neuron9 =neuron9)\n","        \n","        def nn_opt_8(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6,\n","                   neuron7,\n","                   neuron8,\n","                   neuron9):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_8, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_8(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'],\n","                grid_results.best_params_['neuron7'],\n","                grid_results.best_params_['neuron8'],\n","                grid_results.best_params_['neuron9'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==9:       \n","        neuron1 = [16, 32, 64]               \n","        neuron2 = [16, 32, 64]               \n","        neuron3 = [16, 32, 64]   \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        neuron7 = [16, 32, 64] \n","        neuron8 = [16, 32, 64] \n","        neuron9 = [16, 32, 64] \n","        neuron10 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6,\n","                          neuron7=neuron7,\n","                          neuron8=neuron8,\n","                          neuron9=neuron9,\n","                          neuron10=neuron10)\n","        \n","        def nn_opt_9(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6,\n","                   neuron7,\n","                   neuron8,\n","                   neuron9,\n","                   neuron10 ):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron10, input_dim = neuron9, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_9, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_9(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'],\n","                grid_results.best_params_['neuron7'],\n","                grid_results.best_params_['neuron8'],\n","                grid_results.best_params_['neuron9'],\n","                grid_results.best_params_['neuron10'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","    if hLayer==10:       \n","        neuron1 = [16, 32, 64]             \n","        neuron2 = [16, 32, 64]                \n","        neuron3 = [16, 32, 64]   \n","        neuron4 = [16, 32, 64]  \n","        neuron5 = [16, 32, 64] \n","        neuron6 = [16, 32, 64] \n","        neuron7 = [16, 32, 64] \n","        neuron8 = [16, 32, 64] \n","        neuron9 = [16, 32, 64] \n","        neuron10 = [16, 32, 64] \n","        neuron11 = [16, 32, 64] \n","        param_grid = dict(batch_size=batch_size,\n","                          epochs=epochs,\n","                          learn_rate=learn_rate,\n","                          dropout_rate=dropout_rate,\n","                          activation=activation,\n","                          init=init,\n","                          neuron1=neuron1,\n","                          neuron2=neuron2,\n","                          neuron3=neuron3,\n","                          neuron4=neuron4,\n","                          neuron5=neuron5,\n","                          neuron6=neuron6,\n","                          neuron7=neuron7,\n","                          neuron8=neuron8,\n","                          neuron9 =neuron9,\n","                          neuron10=neuron10,\n","                          neuron11 =neuron11)\n","        \n","        def nn_opt_10(activation,\n","                   dropout_rate,\n","                   init,\n","                   learn_rate,\n","                   neuron1,\n","                   neuron2,\n","                   neuron3,\n","                   neuron4,\n","                   neuron5,\n","                   neuron6,\n","                   neuron7,\n","                   neuron8,\n","                   neuron9,\n","                   neuron10,\n","                   neuron11):\n","            model = Sequential()\n","            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron10, input_dim = neuron9, kernel_initializer= init, activation= activation))\n","            model.add(Dense(neuron11, input_dim = neuron10, kernel_initializer= init, activation= activation))\n","            model.add(Dropout(dropout_rate))\n","            model.add(Dense(2, activation='softmax'))\n","            optimizer = Adam(lr = learn_rate)\n","            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","            return model\n","        \n","        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_10, verbose = 0),\n","                            param_grid = param_grid,\n","                            cv = 5,\n","                            n_jobs = -1,\n","                            verbose = 1)\n","        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","        \n","        model =nn_opt_10(grid_results.best_params_['activation'],\n","                grid_results.best_params_['dropout_rate'],\n","                grid_results.best_params_['init'],\n","                grid_results.best_params_['learn_rate'],\n","                grid_results.best_params_['neuron1'],\n","                grid_results.best_params_['neuron2'],\n","                grid_results.best_params_['neuron3'],\n","                grid_results.best_params_['neuron4'],\n","                grid_results.best_params_['neuron5'],\n","                grid_results.best_params_['neuron6'],\n","                grid_results.best_params_['neuron7'],\n","                grid_results.best_params_['neuron8'],\n","                grid_results.best_params_['neuron9'],\n","                grid_results.best_params_['neuron10'],\n","                grid_results.best_params_['neuron11'])\n","        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n","    \n","\n","\n","\n","\n","################################################################ \n","#           this function optimize four hyper-parameters are\n","#           activation,dropout_rate,init,learn_rate            #\n","def nn_opt(activation,dropout_rate,init,learn_rate):\n","    \"\"\"\n","  Parameters :\n","  Input - list of 4 hyper-parameter that are need to\n","  be optimized\n","\n","  Output - Best optimized model \n","    \"\"\"\n","    \n","    #define the optimmized neuron  number from experiment \n","    neuron1,neuron2,neuron3,neuron4=64,16,64,64\n","    ###############################################################################################\n","    # the model building block \n","    \n","    model = Sequential()\n","    np.random.seed(6)\n","    model.add(Dense(neuron1, input_dim =4 , kernel_initializer= init, activation= activation))\n","    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n","    model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n","    model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n","    model.add(Dropout(dropout_rate))\n","    model.add(Dense(2, activation='softmax'))\n","    \n","    optimizer = Adam(lr = learn_rate)               #optimizer of Neural network \n","    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) #compile model \n","    #################################################################################################\n","    return model     \n","\n","\n","    "],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_BXPZol8pem"},"source":["### Read the data from the drive using pandas (Python Data Analysis Library)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"n-q5jMU4z4GA","executionInfo":{"status":"ok","timestamp":1613906888045,"user_tz":-330,"elapsed":988,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}},"outputId":"a15a9a98-63cc-440b-8cdc-04aa60c8ef88"},"source":["data_dir='/content/drive/MyDrive/Diabetes/diabetes.csv'\n","data = pd.read_csv(data_dir)\n","data.shape"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000, 9)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"KluoAwn49hGG","executionInfo":{"status":"ok","timestamp":1613906889268,"user_tz":-330,"elapsed":622,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}},"outputId":"ef4edebf-bba9-479a-b77f-59fb5ae2e4fd"},"source":["data.head(n=6)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>138</td>\n","      <td>62</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.127</td>\n","      <td>47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>84</td>\n","      <td>82</td>\n","      <td>31</td>\n","      <td>125</td>\n","      <td>38.2</td>\n","      <td>0.233</td>\n","      <td>23</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>145</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>44.2</td>\n","      <td>0.630</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>135</td>\n","      <td>68</td>\n","      <td>42</td>\n","      <td>250</td>\n","      <td>42.3</td>\n","      <td>0.365</td>\n","      <td>24</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>139</td>\n","      <td>62</td>\n","      <td>41</td>\n","      <td>480</td>\n","      <td>40.7</td>\n","      <td>0.536</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>173</td>\n","      <td>78</td>\n","      <td>32</td>\n","      <td>265</td>\n","      <td>46.5</td>\n","      <td>1.159</td>\n","      <td>58</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n","0            2      138             62  ...                     0.127   47        1\n","1            0       84             82  ...                     0.233   23        0\n","2            0      145              0  ...                     0.630   31        1\n","3            0      135             68  ...                     0.365   24        1\n","4            1      139             62  ...                     0.536   21        0\n","5            0      173             78  ...                     1.159   58        0\n","\n","[6 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"nwaLj2vN_RmG"},"source":["### Renaming the Features by F1, F2, and so on ......\n"]},{"cell_type":"code","metadata":{"id":"VvHHG7dC_jzd","executionInfo":{"status":"ok","timestamp":1613906892024,"user_tz":-330,"elapsed":849,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":["data = pd.DataFrame({'F1':data.iloc[:,:8].values[:,0],\n","                     'F2':data.iloc[:,:8].values[:,1],\n","                     'F3':data.iloc[:,:8].values[:,2],\n","                     'F4':data.iloc[:,:8].values[:,3],\n","                     'F5':data.iloc[:,:8].values[:,4],\n","                     'F6':data.iloc[:,:8].values[:,5],\n","                     'F7':data.iloc[:,:8].values[:,6],\n","                     'F8':data.iloc[:,:8].values[:,7],\n","                     'Outcome':data.iloc[:,8:].values[:,0]})"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRoQ8jmU-f2A"},"source":["### Show the statistical description of the data which sumarize the central tendency, dispersion, and shape of a data distribution.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"PdYuOfaHtpyn","executionInfo":{"status":"ok","timestamp":1613906895024,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}},"outputId":"bcf3f079-bf02-46ac-dc71-af167e99190f"},"source":["data.describe()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1</th>\n","      <th>F2</th>\n","      <th>F3</th>\n","      <th>F4</th>\n","      <th>F5</th>\n","      <th>F6</th>\n","      <th>F7</th>\n","      <th>F8</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","      <td>2000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3.703500</td>\n","      <td>121.182500</td>\n","      <td>69.145500</td>\n","      <td>20.935000</td>\n","      <td>80.254000</td>\n","      <td>32.193000</td>\n","      <td>0.470930</td>\n","      <td>33.090500</td>\n","      <td>0.342000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>3.306063</td>\n","      <td>32.068636</td>\n","      <td>19.188315</td>\n","      <td>16.103243</td>\n","      <td>111.180534</td>\n","      <td>8.149901</td>\n","      <td>0.323553</td>\n","      <td>11.786423</td>\n","      <td>0.474498</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.078000</td>\n","      <td>21.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>99.000000</td>\n","      <td>63.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>27.375000</td>\n","      <td>0.244000</td>\n","      <td>24.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.000000</td>\n","      <td>117.000000</td>\n","      <td>72.000000</td>\n","      <td>23.000000</td>\n","      <td>40.000000</td>\n","      <td>32.300000</td>\n","      <td>0.376000</td>\n","      <td>29.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>6.000000</td>\n","      <td>141.000000</td>\n","      <td>80.000000</td>\n","      <td>32.000000</td>\n","      <td>130.000000</td>\n","      <td>36.800000</td>\n","      <td>0.624000</td>\n","      <td>40.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>17.000000</td>\n","      <td>199.000000</td>\n","      <td>122.000000</td>\n","      <td>110.000000</td>\n","      <td>744.000000</td>\n","      <td>80.600000</td>\n","      <td>2.420000</td>\n","      <td>81.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                F1           F2  ...           F8      Outcome\n","count  2000.000000  2000.000000  ...  2000.000000  2000.000000\n","mean      3.703500   121.182500  ...    33.090500     0.342000\n","std       3.306063    32.068636  ...    11.786423     0.474498\n","min       0.000000     0.000000  ...    21.000000     0.000000\n","25%       1.000000    99.000000  ...    24.000000     0.000000\n","50%       3.000000   117.000000  ...    29.000000     0.000000\n","75%       6.000000   141.000000  ...    40.000000     1.000000\n","max      17.000000   199.000000  ...    81.000000     1.000000\n","\n","[8 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"_cell_guid":"7790af18-672d-658b-9d0a-58648dd63b13","id":"nluf1OUetXjy"},"source":["# Data Preprocessing  "]},{"cell_type":"code","metadata":{"_cell_guid":"048157d7-1cfc-13b7-feca-5c73b00a03ee","colab":{"base_uri":"https://localhost:8080/","height":0},"id":"jbTObgkctXj1","executionInfo":{"status":"ok","timestamp":1613906903293,"user_tz":-330,"elapsed":2571,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}},"outputId":"5fab46b8-5437-49d9-ba9d-35c2809d6d43"},"source":["# print('Shape Before Process: ' + str(data.shape))\n","\n","# The process for the outlier rejection (P)\n","data = outlier_Rejection (data,\n","                  iqr_Mean=False,\n","                  iqr_Medain=False,\n","                  iqr=True,\n","                  manual=False)\n","print('Shape After outlier Removed: ' + str(data.shape))\n","\n","## The process for the filling missing values (Q)\n","for col in ['F2', 'F3', 'F4', 'F5', 'F6']:   \n","    replace_zero(data, col, 'Outcome')              \n","print('Shape After Filling Missing Value: ' + str(data.shape))\n","     \n","                        \n","X_Data,Y_Lavel = feature_Selector(data, algo='corr', n_feature=4)    \n","print('Shape After Feature Selection: ' + str(X_Data.shape))\n","\n","\n","# scaler =  preprocessing.StandardScaler()\n","# X_Data,Y_Lavel= scaler.fit_transform(X_Data), Y_Lavel\n","# print('Shape After Standardization: ' + str(X_Data.shape))          \n","\n","kf = StratifiedKFold(n_splits=5,\n","                     shuffle=False,\n","                     random_state=random_initializer)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Shape After outlier Removed: (1652, 9)\n","Shape After Filling Missing Value: (1652, 9)\n","Shape After Feature Selection: (1652, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OpAeroXMacWO"},"source":["# MLP MODEl"]},{"cell_type":"markdown","metadata":{"id":"L9kt1_ZOnSWm"},"source":["### MLP Experiment Block\n","\n","A  neural network (NN) is an artificial neural network (ANN) with multiple layers between the input and output layers. The NN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. For the \n","comparison of performance between ML classifiers we find the best peformance using Neural Network.At first we find out the optimum number of hidden layer and number of neuron in every layer. Then, Using grid search technique we optimze the hyperparameters(Number of epoch, batch size, learning rate, droupt out rate , activation afunction , kernel initializer) of NN model .\n"]},{"cell_type":"markdown","metadata":{"id":"pO-9nWCUwz--"},"source":["#### Find the optimum number of hidden layers and neurons in number every layer "]},{"cell_type":"code","metadata":{"id":"Vc7-YnJowbQR","executionInfo":{"status":"ok","timestamp":1613906959009,"user_tz":-330,"elapsed":1051,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":["# ################################\n","# # list for store accuracy and auc value and plot\n","# accuracy = []\n","# AUC = []\n","# ##############################\n","\n","\n","# ################################################################################\n","# # in this block we  optimize number of hiiden layer and neuron number \n","# for hLayer in range(1,11):\n","#     print ('------------------------------Model Running with # of Hidden = '+str(hLayer)+'-----------------------------')\n","#     batch_size, epochs, model, grid_results = run (hLayer,   \n","#                                      batch_size = [32],           \n","#                                      epochs = [100],              \n","#                                      learn_rate = [0.001],         \n","#                                      dropout_rate = [0.3],       \n","#                                      activation = ['relu'],   \n","#                                      init =['normal'])\n","    \n","#     print(grid_results.best_params_)\n","    \n","#     Y_Train_1Hot = to_categorical(Y_Lavel,2)\n","#     model.fit(x=X_Data,\n","#             y=Y_Train_1Hot,\n","#             batch_size=batch_size,\n","#             epochs=epochs,\n","#             verbose=0)\n","#     probas_ = model.predict(X_Data)\n","#     y_pred = np.argmax(model.predict(X_Data), axis=1)\n","    \n","#     #############################################################################\n","#     #   print and stroe the optimized model accuracy and  auc  \n","#     print(accuracy_score(Y_Lavel, y_pred))\n","#     print(roc_auc_score(Y_Train_1Hot, probas_))\n","#     accuracy.append(accuracy_score(Y_Lavel, y_pred))\n","#     AUC.append(roc_auc_score(Y_Train_1Hot, probas_))\n","#     #############################################################################\n","    \n","\n","# ######   plot the value of auc and accuracy ####\n","# plt.plot(AUC)\n","# plt.plot(accuracy)\n","# ################################################"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyVnsEj3RE_M","executionInfo":{"status":"ok","timestamp":1613906960185,"user_tz":-330,"elapsed":581,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":["##########################\n","# Define a random seed\n","seed = 6\n","np.random.seed(seed)\n","###########################\n","\n","\n","\n","# create the model for optimization \n","model = KerasClassifier(build_fn = nn_opt, verbose = 0)\n","\n","##########################################################\n","##List of hyper-parameters of MLP model for optimization##\n","\n","batch_size = [8, 16, 32]            \n","epochs = [100, 150, 200]              \n","learn_rate =[0.001,.05, 0.1]         \n","dropout_rate = [0.0, 0.3, 0.6]       \n","activation = ['relu', 'tanh']   \n","init =['uniform', 'normal']         \n","\n","###########################################################\n","\n","###########################################################\n","## parameter dictionary for grid ssearch   ##\n","param_grid = dict(batch_size=batch_size,\n","                  epochs=epochs,\n","                  learn_rate=learn_rate,\n","                  dropout_rate=dropout_rate,\n","                  activation=activation,\n","                  init=init)\n","\n","#build and fit the GridSearchCV\n","grid = GridSearchCV(estimator = model,\n","                    param_grid = param_grid,\n","                    cv = 5,\n","                    n_jobs = -1,\n","                    verbose = 1)\n","###########################################################\n"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"UPZJ_TGUh9Q_","outputId":"097091b5-a9cd-4ce8-b611-82c34bb24c5d"},"source":["i=0                                                                          # for verification of fold number \n","Accuracy = []                                                                # for store the value of accuracy \n","FP = []                                                                      # for store False Positive \n","TN = []                                                                      # for True Negative\n","FN = []                                                                      # for False Negative\n","TP = []                                                                      # for True Positive\n","tprs = []                                                                    # for true positive rates\n","aucs_ens = []                                                                # for store the values of auc\n","sn = []                                                                      # for sensitivity \n","sp = []                                                                      # for specificity\n","pr = []                                                                      # for precision\n","FOR = []                                                                     # for False omission rate \n","DOR = [] \n","iterator=0\n","mean_fpr = np.linspace(0, 1, 100)\n","\n","\n","#########################################################\n","\n","##      call grid  search for optimization       ##\n","\n","# grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n","   \n","##            tuned/optimized parameters        ##\n","\n","# activation=grid_results.best_params_['activation']\n","# batch_size=grid_results.best_params_['batch_size']\n","# epochs=grid_results.best_params_['epochs']\n","# learn_rate=grid_results.best_params_['learn_rate']\n","# dropout_rate=grid_results.best_params_['dropout_rate']\n","# init=grid_results.best_params_['init']\n","\n","##########################################################\n","  \n","    \n","##########################################################\n","##     best optimizers found  are defined here by experiment ##\n","activation=\"relu\"\n","batch_size=8\n","epochs=200\n","learn_rate=.001\n","dropout_rate=0.6\n","init=\"normal\"\n","neuron1,neuron2,neuron3,neuron4=64,16,64,64\n","############################################################\n","print(activation,batch_size,epochs,learn_rate,dropout_rate,init,\n","    neuron1,neuron2,neuron3,neuron4)\n","\n","\n","\n","for train_index, test_index in kf.split(X_Data,Y_Lavel):                  # for k fold experiment \n","  print('------------------->>>>>>>>>>Fold no = ',i+1)\n","  X_Train, X_Test = X_Data[train_index], X_Data[test_index]               # the train data and label\n","  Y_Train, Y_Test = Y_Lavel[train_index], Y_Lavel[test_index]             # the  test data and label\n","\n","\n","  Y_Train_1Hot = to_categorical(Y_Train,2)                                #convert train output to catagorical\n","  Y_Test_1Hot = to_categorical(Y_Test,2)                                  #convert test output to catagorical\n","\n","  model =nn_opt(activation,                                               #build model using tuned parameters\n","            dropout_rate,\n","            init,\n","            learn_rate)    \n","  np.random.seed(6)\n","  model.fit(x=X_Train,                                                     # fit our model \n","            y=Y_Train_1Hot,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            shuffle=False,\n","            verbose=1)\n","    \n","  probas_ = model.predict(X_Test)                                           #predict the class probability\n","\n","  y_pred = np.argmax(model.predict(X_Test), axis=1)                         #find the max. probability of output class\n","\n","\n","  tn, fp, fn, tp, roc_auc, fpr, tpr = metrics (y_true = Y_Test,           #evaluation parameters of ensembelled model \n","                                              y_pred = y_pred,\n","                                              probas_ = probas_)\n","  tprs.append(interp(mean_fpr, fpr, tpr))\n","  tprs[-1][0] = 0.0\n","  aucs_ens.append(roc_auc)\n","  plot_Current_ROC (fpr,tpr,iterator,roc_auc)                             #plot the ROC curve of current fold\n","  iterator += 1\n","  TN.append(tn)\n","  FP.append(fp)\n","  FN.append(fn)\n","  TP.append(tp)\n","  Accuracy.append(accuracy_score(Y_Test, y_pred))\n","  sn.append(tp/(tp+fn))\n","  sp.append(tn/(fp+tn))\n","  pr.append(tp/(tp+fp))\n","  FOR.append(fn/(tn+fn))\n","  DOR.append((tp*tn)/(fp*fn))\n","  print((tp*tn)/(fp*fn))\n","  i+=1\n","\n","average_ROC(mean_fpr,tprs,aucs_ens,TP,TN,FP,FN)                             #plot average ROC\n","average_performance(aucs_ens,Accuracy,TP,TN,FP,FN)                          #print the average performance  \n","print(\"Sensitivity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sn),np.std(sn)))\n","print(\"Specificity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sp),np.std(sp)))\n","print(\"Precision (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(pr),np.std(pr)))\n","print(\"FOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(FOR),np.std(FOR)))\n","print(\"DOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(DOR),np.std(DOR)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["relu 8 200 0.001 0.6 normal 64 16 64 64\n","------------------->>>>>>>>>>Fold no =  1\n","Epoch 1/200\n","166/166 [==============================] - 2s 2ms/step - loss: 0.6615 - accuracy: 0.6642\n","Epoch 2/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6960\n","Epoch 3/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.6960\n","Epoch 4/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.6960\n","Epoch 5/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.6960\n","Epoch 6/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.6964\n","Epoch 7/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.7173\n","Epoch 8/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7052\n","Epoch 9/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.7021\n","Epoch 10/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.6981\n","Epoch 11/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.7128\n","Epoch 12/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7504\n","Epoch 13/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7812\n","Epoch 14/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8002\n","Epoch 15/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4450 - accuracy: 0.8214\n","Epoch 16/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8038\n","Epoch 17/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8151\n","Epoch 18/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8133\n","Epoch 19/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8312\n","Epoch 20/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8149\n","Epoch 21/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8212\n","Epoch 22/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8220\n","Epoch 23/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8288\n","Epoch 24/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.8213\n","Epoch 25/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8444\n","Epoch 26/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8407\n","Epoch 27/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.8309\n","Epoch 28/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8360\n","Epoch 29/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8178\n","Epoch 30/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8236\n","Epoch 31/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8358\n","Epoch 32/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8386\n","Epoch 33/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8425\n","Epoch 34/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8478\n","Epoch 35/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8361\n","Epoch 36/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8371\n","Epoch 37/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8508\n","Epoch 38/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8451\n","Epoch 39/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8637\n","Epoch 40/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.8529\n","Epoch 41/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8585\n","Epoch 42/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8522\n","Epoch 43/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8500\n","Epoch 44/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8554\n","Epoch 45/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8534\n","Epoch 46/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.8557\n","Epoch 47/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8528\n","Epoch 48/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8677\n","Epoch 49/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8610\n","Epoch 50/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8497\n","Epoch 51/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8628\n","Epoch 52/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8536\n","Epoch 53/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8643\n","Epoch 54/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8577\n","Epoch 55/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8591\n","Epoch 56/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8452\n","Epoch 57/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8507\n","Epoch 58/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8497\n","Epoch 59/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8602\n","Epoch 60/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8556\n","Epoch 61/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8632\n","Epoch 62/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8727\n","Epoch 63/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8416\n","Epoch 64/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8537\n","Epoch 65/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8588\n","Epoch 66/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8537\n","Epoch 67/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8666\n","Epoch 68/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8605\n","Epoch 69/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8601\n","Epoch 70/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8492\n","Epoch 71/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8534\n","Epoch 72/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8543\n","Epoch 73/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8611\n","Epoch 74/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8539\n","Epoch 75/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8487\n","Epoch 76/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8500\n","Epoch 77/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8642\n","Epoch 78/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8524\n","Epoch 79/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8645\n","Epoch 80/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8461\n","Epoch 81/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8517\n","Epoch 82/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8492\n","Epoch 83/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8531\n","Epoch 84/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8558\n","Epoch 85/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8636\n","Epoch 86/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8575\n","Epoch 87/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8540\n","Epoch 88/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.8626\n","Epoch 89/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8627\n","Epoch 90/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8554\n","Epoch 91/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.8520\n","Epoch 92/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8649\n","Epoch 93/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8591\n","Epoch 94/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3172 - accuracy: 0.8651\n","Epoch 95/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8482\n","Epoch 96/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8694\n","Epoch 97/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.8647\n","Epoch 98/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8648\n","Epoch 99/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8516\n","Epoch 100/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8700\n","Epoch 101/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.8624\n","Epoch 102/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8722\n","Epoch 103/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8645\n","Epoch 104/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8646\n","Epoch 105/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.8512\n","Epoch 106/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8610\n","Epoch 107/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8659\n","Epoch 108/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8676\n","Epoch 109/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8559\n","Epoch 110/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8613\n","Epoch 111/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8682\n","Epoch 112/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8556\n","Epoch 113/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8697\n","Epoch 114/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8648\n","Epoch 115/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8466\n","Epoch 116/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8642\n","Epoch 117/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8688\n","Epoch 118/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8607\n","Epoch 119/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8690\n","Epoch 120/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8669\n","Epoch 121/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8737\n","Epoch 122/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8732\n","Epoch 123/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8614\n","Epoch 124/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8707\n","Epoch 125/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8684\n","Epoch 126/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8691\n","Epoch 127/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8610\n","Epoch 128/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8646\n","Epoch 129/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8670\n","Epoch 130/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8610\n","Epoch 131/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8708\n","Epoch 132/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8749\n","Epoch 133/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8739\n","Epoch 134/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8837\n","Epoch 135/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8688\n","Epoch 136/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8712\n","Epoch 137/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.8800\n","Epoch 138/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8758\n","Epoch 139/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8717\n","Epoch 140/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8792\n","Epoch 141/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8620\n","Epoch 142/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8710\n","Epoch 143/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.8692\n","Epoch 144/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8790\n","Epoch 145/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8686\n","Epoch 146/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8710\n","Epoch 147/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8698\n","Epoch 148/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8744\n","Epoch 149/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8754\n","Epoch 150/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8718\n","Epoch 151/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8797\n","Epoch 152/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8682\n","Epoch 153/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8771\n","Epoch 154/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8727\n","Epoch 155/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8828\n","Epoch 156/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8782\n","Epoch 157/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8783\n","Epoch 158/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8622\n","Epoch 159/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.8688\n","Epoch 160/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8750\n","Epoch 161/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8645\n","Epoch 162/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8693\n","Epoch 163/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8718\n","Epoch 164/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8723\n","Epoch 165/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8713\n","Epoch 166/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8621\n","Epoch 167/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8617\n","Epoch 168/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8712\n","Epoch 169/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8730\n","Epoch 170/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8663\n","Epoch 171/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3111 - accuracy: 0.8663\n","Epoch 172/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8737\n","Epoch 173/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8654\n","Epoch 174/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8686\n","Epoch 175/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8708\n","Epoch 176/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.8689\n","Epoch 177/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8644\n","Epoch 178/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8738\n","Epoch 179/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8791\n","Epoch 180/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8745\n","Epoch 181/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8754\n","Epoch 182/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8587\n","Epoch 183/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8686\n","Epoch 184/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8744\n","Epoch 185/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3183 - accuracy: 0.8606\n","Epoch 186/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8707\n","Epoch 187/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8710\n","Epoch 188/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8766\n","Epoch 189/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.8744\n","Epoch 190/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.8713\n","Epoch 191/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8745\n","Epoch 192/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8755\n","Epoch 193/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.8594\n","Epoch 194/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8721\n","Epoch 195/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.8711\n","Epoch 196/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8698\n","Epoch 197/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8716\n","Epoch 198/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8729\n","Epoch 199/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8624\n","Epoch 200/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8815\n","--------------------------------------------------\n","Detailed classification report for current fold:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.96      0.91       229\n","           1       0.88      0.69      0.77       102\n","\n","    accuracy                           0.87       331\n","   macro avg       0.87      0.82      0.84       331\n","weighted avg       0.87      0.87      0.87       331\n","\n","\n","Area Under ROC (AUC): 0.9399777378200189\n","\n","Confusion Matrix for current fold: \n","[[219  10]\n"," [ 32  70]]\n","\n","Accuracy for Current Fold: 0.8731117824773413\n","\n","47.90625\n","------------------->>>>>>>>>>Fold no =  2\n","Epoch 1/200\n","166/166 [==============================] - 1s 2ms/step - loss: 0.6501 - accuracy: 0.6900\n","Epoch 2/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7173\n","Epoch 3/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7322\n","Epoch 4/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7617\n","Epoch 5/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7770\n","Epoch 6/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.8035\n","Epoch 7/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8062\n","Epoch 8/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8318\n","Epoch 9/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8345\n","Epoch 10/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8208\n","Epoch 11/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8326\n","Epoch 12/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8337\n","Epoch 13/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8359\n","Epoch 14/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8309\n","Epoch 15/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8413\n","Epoch 16/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8483\n","Epoch 17/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8413\n","Epoch 18/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8400\n","Epoch 19/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.8390\n","Epoch 20/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8390\n","Epoch 21/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8462\n","Epoch 22/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8470\n","Epoch 23/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8273\n","Epoch 24/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8542\n","Epoch 25/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8489\n","Epoch 26/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.8549\n","Epoch 27/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8435\n","Epoch 28/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3564 - accuracy: 0.8576\n","Epoch 29/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8339\n","Epoch 30/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.8491\n","Epoch 31/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8542\n","Epoch 32/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8522\n","Epoch 33/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8523\n","Epoch 34/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8596\n","Epoch 35/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8539\n","Epoch 36/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8531\n","Epoch 37/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8475\n","Epoch 38/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8497\n","Epoch 39/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8452\n","Epoch 40/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8528\n","Epoch 41/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8397\n","Epoch 42/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8471\n","Epoch 43/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8550\n","Epoch 44/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8467\n","Epoch 45/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8593\n","Epoch 46/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8594\n","Epoch 47/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8571\n","Epoch 48/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8547\n","Epoch 49/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8530\n","Epoch 50/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8554\n","Epoch 51/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.8513\n","Epoch 52/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8601\n","Epoch 53/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8570\n","Epoch 54/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8562\n","Epoch 55/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8609\n","Epoch 56/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8523\n","Epoch 57/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8472\n","Epoch 58/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8443\n","Epoch 59/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3411 - accuracy: 0.8457\n","Epoch 60/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8513\n","Epoch 61/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8550\n","Epoch 62/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.8543\n","Epoch 63/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8511\n","Epoch 64/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8493\n","Epoch 65/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3469 - accuracy: 0.8612\n","Epoch 66/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8542\n","Epoch 67/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8629\n","Epoch 68/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8600\n","Epoch 69/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8650\n","Epoch 70/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8645\n","Epoch 71/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8646\n","Epoch 72/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8515\n","Epoch 73/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8446\n","Epoch 74/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8580\n","Epoch 75/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8537\n","Epoch 76/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8584\n","Epoch 77/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8584\n","Epoch 78/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8680\n","Epoch 79/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8585\n","Epoch 80/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8574\n","Epoch 81/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8565\n","Epoch 82/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8588\n","Epoch 83/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8730\n","Epoch 84/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8605\n","Epoch 85/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8676\n","Epoch 86/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8597\n","Epoch 87/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8586\n","Epoch 88/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8736\n","Epoch 89/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3044 - accuracy: 0.8689\n","Epoch 90/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8710\n","Epoch 91/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8703\n","Epoch 92/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8727\n","Epoch 93/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8722\n","Epoch 94/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8689\n","Epoch 95/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8672\n","Epoch 96/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8721\n","Epoch 97/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8702\n","Epoch 98/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8741\n","Epoch 99/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8618\n","Epoch 100/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8771\n","Epoch 101/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.8821\n","Epoch 102/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8595\n","Epoch 103/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8738\n","Epoch 104/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8787\n","Epoch 105/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8698\n","Epoch 106/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.8859\n","Epoch 107/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8739\n","Epoch 108/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8792\n","Epoch 109/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8701\n","Epoch 110/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8700\n","Epoch 111/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8785\n","Epoch 112/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8707\n","Epoch 113/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8734\n","Epoch 114/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8795\n","Epoch 115/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8614\n","Epoch 116/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8644\n","Epoch 117/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8711\n","Epoch 118/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.8818\n","Epoch 119/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8761\n","Epoch 120/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2866 - accuracy: 0.8772\n","Epoch 121/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8829\n","Epoch 122/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8821\n","Epoch 123/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8781\n","Epoch 124/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.8769\n","Epoch 125/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8696\n","Epoch 126/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8735\n","Epoch 127/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8681\n","Epoch 128/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8564\n","Epoch 129/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.8763\n","Epoch 130/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8719\n","Epoch 131/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8717\n","Epoch 132/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8716\n","Epoch 133/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8699\n","Epoch 134/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8711\n","Epoch 135/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8772\n","Epoch 136/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.8842\n","Epoch 137/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.8837\n","Epoch 138/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8723\n","Epoch 139/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8584\n","Epoch 140/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.8809\n","Epoch 141/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8864\n","Epoch 142/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8590\n","Epoch 143/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8677\n","Epoch 144/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8686\n","Epoch 145/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.8807\n","Epoch 146/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8706\n","Epoch 147/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8735\n","Epoch 148/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.8820\n","Epoch 149/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8770\n","Epoch 150/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8778\n","Epoch 151/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8753\n","Epoch 152/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8762\n","Epoch 153/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8633\n","Epoch 154/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8654\n","Epoch 155/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8664\n","Epoch 156/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.8726\n","Epoch 157/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8795\n","Epoch 158/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.8872\n","Epoch 159/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8751\n","Epoch 160/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2884 - accuracy: 0.8769\n","Epoch 161/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8718\n","Epoch 162/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8728\n","Epoch 163/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.8840\n","Epoch 164/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8682\n","Epoch 165/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.8676\n","Epoch 166/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8824\n","Epoch 167/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8775\n","Epoch 168/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8695\n","Epoch 169/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.8751\n","Epoch 170/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8824\n","Epoch 171/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.8783\n","Epoch 172/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8762\n","Epoch 173/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8725\n","Epoch 174/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8734\n","Epoch 175/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8783\n","Epoch 176/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.8822\n","Epoch 177/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8816\n","Epoch 178/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8723\n","Epoch 179/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8738\n","Epoch 180/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2859 - accuracy: 0.8848\n","Epoch 181/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8672\n","Epoch 182/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8647\n","Epoch 183/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.8889\n","Epoch 184/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.2605 - accuracy: 0.8862\n","Epoch 185/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8783\n","Epoch 186/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8791\n","Epoch 187/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8749\n","Epoch 188/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8738\n","Epoch 189/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8878\n","Epoch 190/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8578\n","Epoch 191/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8759\n","Epoch 192/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8927\n","Epoch 193/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8751\n","Epoch 194/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8673\n","Epoch 195/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.8561\n","Epoch 196/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8775\n","Epoch 197/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8756\n","Epoch 198/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8765\n","Epoch 199/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8804\n","Epoch 200/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8869\n","--------------------------------------------------\n","Detailed classification report for current fold:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.99      0.92       229\n","           1       0.97      0.61      0.75       102\n","\n","    accuracy                           0.87       331\n","   macro avg       0.91      0.80      0.83       331\n","weighted avg       0.89      0.87      0.86       331\n","\n","\n","Area Under ROC (AUC): 0.9403202328966521\n","\n","Confusion Matrix for current fold: \n","[[227   2]\n"," [ 40  62]]\n","\n","Accuracy for Current Fold: 0.8731117824773413\n","\n","175.925\n","------------------->>>>>>>>>>Fold no =  3\n","Epoch 1/200\n","166/166 [==============================] - 1s 2ms/step - loss: 0.6633 - accuracy: 0.6502\n","Epoch 2/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.6958\n","Epoch 3/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5534 - accuracy: 0.7118\n","Epoch 4/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7255\n","Epoch 5/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7553\n","Epoch 6/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7585\n","Epoch 7/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.7665\n","Epoch 8/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.7840\n","Epoch 9/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.7938\n","Epoch 10/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.7978\n","Epoch 11/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8065\n","Epoch 12/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8242\n","Epoch 13/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4036 - accuracy: 0.8307\n","Epoch 14/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8001\n","Epoch 15/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4177 - accuracy: 0.8258\n","Epoch 16/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8268\n","Epoch 17/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8371\n","Epoch 18/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8414\n","Epoch 19/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.8293\n","Epoch 20/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8333\n","Epoch 21/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8348\n","Epoch 22/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8320\n","Epoch 23/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8372\n","Epoch 24/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8434\n","Epoch 25/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8347\n","Epoch 26/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8507\n","Epoch 27/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8416\n","Epoch 28/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8372\n","Epoch 29/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8429\n","Epoch 30/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8373\n","Epoch 31/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8316\n","Epoch 32/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8447\n","Epoch 33/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8472\n","Epoch 34/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8396\n","Epoch 35/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8433\n","Epoch 36/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8443\n","Epoch 37/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8535\n","Epoch 38/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8480\n","Epoch 39/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8502\n","Epoch 40/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8444\n","Epoch 41/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8431\n","Epoch 42/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8451\n","Epoch 43/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8465\n","Epoch 44/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8460\n","Epoch 45/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8497\n","Epoch 46/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8421\n","Epoch 47/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8473\n","Epoch 48/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8541\n","Epoch 49/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8465\n","Epoch 50/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8496\n","Epoch 51/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8537\n","Epoch 52/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8445\n","Epoch 53/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8555\n","Epoch 54/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8428\n","Epoch 55/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8523\n","Epoch 56/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8526\n","Epoch 57/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8517\n","Epoch 58/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8482\n","Epoch 59/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8491\n","Epoch 60/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8467\n","Epoch 61/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8461\n","Epoch 62/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8524\n","Epoch 63/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8553\n","Epoch 64/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8586\n","Epoch 65/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8528\n","Epoch 66/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.8539\n","Epoch 67/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8497\n","Epoch 68/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8501\n","Epoch 69/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8467\n","Epoch 70/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8372\n","Epoch 71/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8533\n","Epoch 72/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8393\n","Epoch 73/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8443\n","Epoch 74/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8518\n","Epoch 75/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8456\n","Epoch 76/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8525\n","Epoch 77/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8476\n","Epoch 78/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8510\n","Epoch 79/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8485\n","Epoch 80/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8430\n","Epoch 81/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8411\n","Epoch 82/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8540\n","Epoch 83/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8488\n","Epoch 84/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8529\n","Epoch 85/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8535\n","Epoch 86/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8529\n","Epoch 87/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8427\n","Epoch 88/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8468\n","Epoch 89/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8498\n","Epoch 90/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8484\n","Epoch 91/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8456\n","Epoch 92/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8448\n","Epoch 93/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8513\n","Epoch 94/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8392\n","Epoch 95/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8400\n","Epoch 96/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8513\n","Epoch 97/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8446\n","Epoch 98/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8475\n","Epoch 99/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8479\n","Epoch 100/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8403\n","Epoch 101/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8476\n","Epoch 102/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8377\n","Epoch 103/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8483\n","Epoch 104/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8512\n","Epoch 105/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8511\n","Epoch 106/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8482\n","Epoch 107/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8442\n","Epoch 108/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8434\n","Epoch 109/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8425\n","Epoch 110/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8490\n","Epoch 111/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3696 - accuracy: 0.8499\n","Epoch 112/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8425\n","Epoch 113/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.8436\n","Epoch 114/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3506 - accuracy: 0.8521\n","Epoch 115/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3620 - accuracy: 0.8477\n","Epoch 116/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3632 - accuracy: 0.8448\n","Epoch 117/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8556\n","Epoch 118/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8468\n","Epoch 119/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8492\n","Epoch 120/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8507\n","Epoch 121/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.8412\n","Epoch 122/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8433\n","Epoch 123/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8500\n","Epoch 124/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8408\n","Epoch 125/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8491\n","Epoch 126/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8539\n","Epoch 127/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8456\n","Epoch 128/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.8432\n","Epoch 129/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8446\n","Epoch 130/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8441\n","Epoch 131/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8489\n","Epoch 132/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8426\n","Epoch 133/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8505\n","Epoch 134/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8367\n","Epoch 135/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8538\n","Epoch 136/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3785 - accuracy: 0.8492\n","Epoch 137/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8388\n","Epoch 138/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8523\n","Epoch 139/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8428\n","Epoch 140/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8493\n","Epoch 141/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8502\n","Epoch 142/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8449\n","Epoch 143/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8531\n","Epoch 144/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8550\n","Epoch 145/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8570\n","Epoch 146/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8529\n","Epoch 147/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8530\n","Epoch 148/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.8567\n","Epoch 149/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8522\n","Epoch 150/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8517\n","Epoch 151/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8546\n","Epoch 152/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8488\n","Epoch 153/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8550\n","Epoch 154/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8468\n","Epoch 155/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8532\n","Epoch 156/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.8557\n","Epoch 157/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8576\n","Epoch 158/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8541\n","Epoch 159/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8578\n","Epoch 160/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8511\n","Epoch 161/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8479\n","Epoch 162/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8484\n","Epoch 163/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8586\n","Epoch 164/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8525\n","Epoch 165/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8587\n","Epoch 166/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8564\n","Epoch 167/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8535\n","Epoch 168/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8562\n","Epoch 169/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8580\n","Epoch 170/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8470\n","Epoch 171/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3546 - accuracy: 0.8599\n","Epoch 172/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8526\n","Epoch 173/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8531\n","Epoch 174/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8497\n","Epoch 175/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8492\n","Epoch 176/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8521\n","Epoch 177/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8544\n","Epoch 178/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8627\n","Epoch 179/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8551\n","Epoch 180/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.8570\n","Epoch 181/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8565\n","Epoch 182/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8512\n","Epoch 183/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8518\n","Epoch 184/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8565\n","Epoch 185/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8674\n","Epoch 186/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8612\n","Epoch 187/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8588\n","Epoch 188/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8514\n","Epoch 189/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8494\n","Epoch 190/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8560\n","Epoch 191/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8562\n","Epoch 192/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8559\n","Epoch 193/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8585\n","Epoch 194/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8558\n","Epoch 195/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8635\n","Epoch 196/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8505\n","Epoch 197/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8560\n","Epoch 198/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8579\n","Epoch 199/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8593\n","Epoch 200/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8538\n","--------------------------------------------------\n","Detailed classification report for current fold:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.86      0.89       229\n","           1       0.72      0.83      0.77       101\n","\n","    accuracy                           0.85       330\n","   macro avg       0.82      0.85      0.83       330\n","weighted avg       0.86      0.85      0.85       330\n","\n","\n","Area Under ROC (AUC): 0.8980933027800597\n","\n","Confusion Matrix for current fold: \n","[[197  32]\n"," [ 17  84]]\n","\n","Accuracy for Current Fold: 0.8515151515151516\n","\n","30.419117647058822\n","------------------->>>>>>>>>>Fold no =  4\n","Epoch 1/200\n","166/166 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6796\n","Epoch 2/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7149\n","Epoch 3/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7411\n","Epoch 4/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7599\n","Epoch 5/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7532\n","Epoch 6/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7587\n","Epoch 7/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.7653\n","Epoch 8/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7879\n","Epoch 9/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7887\n","Epoch 10/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4140 - accuracy: 0.8206\n","Epoch 11/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.8093\n","Epoch 12/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.7936\n","Epoch 13/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4050 - accuracy: 0.8163\n","Epoch 14/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8119\n","Epoch 15/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8137\n","Epoch 16/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8071\n","Epoch 17/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8199\n","Epoch 18/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8283\n","Epoch 19/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.8184\n","Epoch 20/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4049 - accuracy: 0.8232\n","Epoch 21/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8349\n","Epoch 22/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8357\n","Epoch 23/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8193\n","Epoch 24/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8304\n","Epoch 25/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8263\n","Epoch 26/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8392\n","Epoch 27/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8298\n","Epoch 28/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8321\n","Epoch 29/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8084\n","Epoch 30/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8245\n","Epoch 31/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8400\n","Epoch 32/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4215 - accuracy: 0.8212\n","Epoch 33/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8270\n","Epoch 34/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8256\n","Epoch 35/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8271\n","Epoch 36/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8289\n","Epoch 37/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8217\n","Epoch 38/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8253\n","Epoch 39/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8363\n","Epoch 40/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8342\n","Epoch 41/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 0.8273\n","Epoch 42/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3757 - accuracy: 0.8363\n","Epoch 43/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8179\n","Epoch 44/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8339\n","Epoch 45/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8305\n","Epoch 46/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8319\n","Epoch 47/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3541 - accuracy: 0.8421\n","Epoch 48/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8426\n","Epoch 49/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3601 - accuracy: 0.8330\n","Epoch 50/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8350\n","Epoch 51/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8302\n","Epoch 52/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8283\n","Epoch 53/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.8235\n","Epoch 54/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3616 - accuracy: 0.8484\n","Epoch 55/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8396\n","Epoch 56/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8384\n","Epoch 57/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8409\n","Epoch 58/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8500\n","Epoch 59/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8419\n","Epoch 60/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8490\n","Epoch 61/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8386\n","Epoch 62/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8498\n","Epoch 63/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8463\n","Epoch 64/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.8457\n","Epoch 65/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8460\n","Epoch 66/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8425\n","Epoch 67/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3417 - accuracy: 0.8431\n","Epoch 68/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8336\n","Epoch 69/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8446\n","Epoch 70/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8489\n","Epoch 71/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3488 - accuracy: 0.8377\n","Epoch 72/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8396\n","Epoch 73/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8400\n","Epoch 74/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8379\n","Epoch 75/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8431\n","Epoch 76/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8471\n","Epoch 77/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8517\n","Epoch 78/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8420\n","Epoch 79/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8494\n","Epoch 80/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8458\n","Epoch 81/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8422\n","Epoch 82/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8529\n","Epoch 83/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8442\n","Epoch 84/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8399\n","Epoch 85/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8452\n","Epoch 86/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8408\n","Epoch 87/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8443\n","Epoch 88/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8489\n","Epoch 89/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3502 - accuracy: 0.8349\n","Epoch 90/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8447\n","Epoch 91/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8432\n","Epoch 92/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8444\n","Epoch 93/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8419\n","Epoch 94/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8469\n","Epoch 95/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8533\n","Epoch 96/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8436\n","Epoch 97/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8448\n","Epoch 98/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8442\n","Epoch 99/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8472\n","Epoch 100/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3344 - accuracy: 0.8441\n","Epoch 101/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8513\n","Epoch 102/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8421\n","Epoch 103/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8459\n","Epoch 104/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8425\n","Epoch 105/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8392\n","Epoch 106/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8469\n","Epoch 107/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8423\n","Epoch 108/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8392\n","Epoch 109/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8411\n","Epoch 110/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8536\n","Epoch 111/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8440\n","Epoch 112/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8512\n","Epoch 113/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8484\n","Epoch 114/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8487\n","Epoch 115/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8498\n","Epoch 116/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8511\n","Epoch 117/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8565\n","Epoch 118/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3502 - accuracy: 0.8561\n","Epoch 119/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8471\n","Epoch 120/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8387\n","Epoch 121/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8472\n","Epoch 122/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3277 - accuracy: 0.8512\n","Epoch 123/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8448\n","Epoch 124/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8521\n","Epoch 125/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8506\n","Epoch 126/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8438\n","Epoch 127/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8457\n","Epoch 128/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8440\n","Epoch 129/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8404\n","Epoch 130/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8479\n","Epoch 131/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8499\n","Epoch 132/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8552\n","Epoch 133/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8574\n","Epoch 134/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8493\n","Epoch 135/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8544\n","Epoch 136/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8529\n","Epoch 137/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8470\n","Epoch 138/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8514\n","Epoch 139/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8534\n","Epoch 140/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3401 - accuracy: 0.8539\n","Epoch 141/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8537\n","Epoch 142/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8448\n","Epoch 143/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8491\n","Epoch 144/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8491\n","Epoch 145/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8462\n","Epoch 146/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3476 - accuracy: 0.8452\n","Epoch 147/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8516\n","Epoch 148/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3151 - accuracy: 0.8514\n","Epoch 149/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8391\n","Epoch 150/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8531\n","Epoch 151/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8541\n","Epoch 152/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8506\n","Epoch 153/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8553\n","Epoch 154/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8520\n","Epoch 155/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3553 - accuracy: 0.8416\n","Epoch 156/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8513\n","Epoch 157/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8467\n","Epoch 158/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8576\n","Epoch 159/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8534\n","Epoch 160/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8385\n","Epoch 161/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8448\n","Epoch 162/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8513\n","Epoch 163/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8504\n","Epoch 164/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3143 - accuracy: 0.8520\n","Epoch 165/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8530\n","Epoch 166/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8485\n","Epoch 167/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3186 - accuracy: 0.8546\n","Epoch 168/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8471\n","Epoch 169/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8476\n","Epoch 170/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8589\n","Epoch 171/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8516\n","Epoch 172/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8600\n","Epoch 173/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8512\n","Epoch 174/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8576\n","Epoch 175/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8546\n","Epoch 176/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8546\n","Epoch 177/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8589\n","Epoch 178/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8541\n","Epoch 179/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8537\n","Epoch 180/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8490\n","Epoch 181/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3537 - accuracy: 0.8431\n","Epoch 182/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8604\n","Epoch 183/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8461\n","Epoch 184/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8549\n","Epoch 185/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8518\n","Epoch 186/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3182 - accuracy: 0.8576\n","Epoch 187/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8570\n","Epoch 188/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8574\n","Epoch 189/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8455\n","Epoch 190/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8582\n","Epoch 191/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8422\n","Epoch 192/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8503\n","Epoch 193/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8473\n","Epoch 194/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3054 - accuracy: 0.8558\n","Epoch 195/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8342\n","Epoch 196/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8349\n","Epoch 197/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8570\n","Epoch 198/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8531\n","Epoch 199/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8460\n","Epoch 200/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8496\n","--------------------------------------------------\n","Detailed classification report for current fold:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.93      0.89       229\n","           1       0.80      0.65      0.72       101\n","\n","    accuracy                           0.85       330\n","   macro avg       0.83      0.79      0.81       330\n","weighted avg       0.84      0.85      0.84       330\n","\n","\n","Area Under ROC (AUC): 0.9306498335423061\n","\n","Confusion Matrix for current fold: \n","[[213  16]\n"," [ 35  66]]\n","\n","Accuracy for Current Fold: 0.8454545454545455\n","\n","25.103571428571428\n","------------------->>>>>>>>>>Fold no =  5\n","Epoch 1/200\n","166/166 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6685\n","Epoch 2/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.7183\n","Epoch 3/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5308 - accuracy: 0.7511\n","Epoch 4/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7664\n","Epoch 5/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4943 - accuracy: 0.7861\n","Epoch 6/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4663 - accuracy: 0.7949\n","Epoch 7/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8128\n","Epoch 8/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8203\n","Epoch 9/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8206\n","Epoch 10/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.8234\n","Epoch 11/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8333\n","Epoch 12/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8273\n","Epoch 13/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8532\n","Epoch 14/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8319\n","Epoch 15/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8320\n","Epoch 16/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4158 - accuracy: 0.8188\n","Epoch 17/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8257\n","Epoch 18/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8325\n","Epoch 19/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8363\n","Epoch 20/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8361\n","Epoch 21/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8351\n","Epoch 22/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8353\n","Epoch 23/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8340\n","Epoch 24/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8311\n","Epoch 25/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8453\n","Epoch 26/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8358\n","Epoch 27/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8427\n","Epoch 28/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8392\n","Epoch 29/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8380\n","Epoch 30/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8389\n","Epoch 31/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8349\n","Epoch 32/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.4080 - accuracy: 0.8465\n","Epoch 33/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8422\n","Epoch 34/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8384\n","Epoch 35/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8376\n","Epoch 36/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8357\n","Epoch 37/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3796 - accuracy: 0.8468\n","Epoch 38/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8388\n","Epoch 39/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8329\n","Epoch 40/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8114\n","Epoch 41/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8315\n","Epoch 42/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8396\n","Epoch 43/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8324\n","Epoch 44/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8489\n","Epoch 45/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3781 - accuracy: 0.8388\n","Epoch 46/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8427\n","Epoch 47/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8490\n","Epoch 48/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8285\n","Epoch 49/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3651 - accuracy: 0.8434\n","Epoch 50/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8455\n","Epoch 51/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8315\n","Epoch 52/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8341\n","Epoch 53/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8361\n","Epoch 54/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8370\n","Epoch 55/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8351\n","Epoch 56/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8426\n","Epoch 57/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8389\n","Epoch 58/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8445\n","Epoch 59/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3781 - accuracy: 0.8481\n","Epoch 60/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8476\n","Epoch 61/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8436\n","Epoch 62/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8419\n","Epoch 63/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8445\n","Epoch 64/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8385\n","Epoch 65/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8426\n","Epoch 66/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8479\n","Epoch 67/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 0.8408\n","Epoch 68/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8438\n","Epoch 69/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8453\n","Epoch 70/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8445\n","Epoch 71/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8366\n","Epoch 72/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8422\n","Epoch 73/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8418\n","Epoch 74/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3837 - accuracy: 0.8444\n","Epoch 75/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8389\n","Epoch 76/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8402\n","Epoch 77/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8467\n","Epoch 78/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8436\n","Epoch 79/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.8482\n","Epoch 80/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8379\n","Epoch 81/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3737 - accuracy: 0.8387\n","Epoch 82/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8456\n","Epoch 83/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3631 - accuracy: 0.8480\n","Epoch 84/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8452\n","Epoch 85/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8407\n","Epoch 86/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8417\n","Epoch 87/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8452\n","Epoch 88/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8458\n","Epoch 89/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3547 - accuracy: 0.8423\n","Epoch 90/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8426\n","Epoch 91/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8461\n","Epoch 92/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8478\n","Epoch 93/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8386\n","Epoch 94/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8481\n","Epoch 95/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8474\n","Epoch 96/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8380\n","Epoch 97/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8469\n","Epoch 98/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8398\n","Epoch 99/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8462\n","Epoch 100/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8470\n","Epoch 101/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8401\n","Epoch 102/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8407\n","Epoch 103/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.8431\n","Epoch 104/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8368\n","Epoch 105/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8444\n","Epoch 106/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8383\n","Epoch 107/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8407\n","Epoch 108/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3500 - accuracy: 0.8451\n","Epoch 109/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8415\n","Epoch 110/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8472\n","Epoch 111/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8591\n","Epoch 112/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8479\n","Epoch 113/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8530\n","Epoch 114/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8601\n","Epoch 115/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8514\n","Epoch 116/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8447\n","Epoch 117/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8500\n","Epoch 118/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8604\n","Epoch 119/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8559\n","Epoch 120/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8557\n","Epoch 121/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8685\n","Epoch 122/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8561\n","Epoch 123/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.8576\n","Epoch 124/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8600\n","Epoch 125/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8524\n","Epoch 126/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.8620\n","Epoch 127/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8538\n","Epoch 128/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.8508\n","Epoch 129/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.8500\n","Epoch 130/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8582\n","Epoch 131/200\n","166/166 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8516\n","Epoch 132/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8615\n","Epoch 133/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8670\n","Epoch 134/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8568\n","Epoch 135/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8619\n","Epoch 136/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8580\n","Epoch 137/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3267 - accuracy: 0.8672\n","Epoch 138/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8562\n","Epoch 139/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8532\n","Epoch 140/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.8624\n","Epoch 141/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.8585\n","Epoch 142/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8534\n","Epoch 143/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8543\n","Epoch 144/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3428 - accuracy: 0.8615\n","Epoch 145/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8630\n","Epoch 146/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8446\n","Epoch 147/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8586\n","Epoch 148/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8594\n","Epoch 149/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8504\n","Epoch 150/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8533\n","Epoch 151/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3570 - accuracy: 0.8427\n","Epoch 152/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8569\n","Epoch 153/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8620\n","Epoch 154/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3429 - accuracy: 0.8470\n","Epoch 155/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8412\n","Epoch 156/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8566\n","Epoch 157/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8546\n","Epoch 158/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8607\n","Epoch 159/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8651\n","Epoch 160/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8587\n","Epoch 161/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8532\n","Epoch 162/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8601\n","Epoch 163/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8634\n","Epoch 164/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8550\n","Epoch 165/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8568\n","Epoch 166/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8618\n","Epoch 167/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8518\n","Epoch 168/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8625\n","Epoch 169/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8526\n","Epoch 170/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8683\n","Epoch 171/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8570\n","Epoch 172/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8531\n","Epoch 173/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.8594\n","Epoch 174/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8600\n","Epoch 175/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8570\n","Epoch 176/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8599\n","Epoch 177/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8467\n","Epoch 178/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8507\n","Epoch 179/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8458\n","Epoch 180/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8544\n","Epoch 181/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8540\n","Epoch 182/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8505\n","Epoch 183/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8544\n","Epoch 184/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8637\n","Epoch 185/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8538\n","Epoch 186/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8609\n","Epoch 187/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8612\n","Epoch 188/200\n","166/166 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8577\n","Epoch 189/200\n"," 37/166 [=====>........................] - ETA: 0s - loss: 0.3563 - accuracy: 0.8440"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sGJRJI8fUx59","executionInfo":{"status":"ok","timestamp":1613906568138,"user_tz":-330,"elapsed":1172,"user":{"displayName":"Project Covid-19","photoUrl":"","userId":"02578458095516687503"}}},"source":[""],"execution_count":22,"outputs":[]}]}